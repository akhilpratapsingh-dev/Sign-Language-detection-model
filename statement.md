ğŸ§© Problem Statement

Communication between hearing-impaired individuals and people who do not understand sign language creates a significant barrier in daily life, education, workplaces, and public services.
Traditional sign language interpreters are not always available, which makes real-time communication difficult.

There is a need for a low-cost, accessible, and real-time system that can understand hand gestures and translate them into readable output.

This project solves this problem by creating a machine learning-based Sign Language Detection System that recognizes Aâ€“Z hand signs using a webcam.

ğŸ¯ Scope of the Project

The system detects and classifies static hand gestures representing Aâ€“Z alphabets.

The project focuses on single-hand gestures only.

Real-time video feed is processed through webcam.

Uses MediaPipe for hand landmark detection and ML model (Random Forest) for classification.

Output is displayed on the screen in real time.

Data collection, dataset creation, and model training scripts are included.

Note: This project does not cover full sentences or dynamic gestures; only static alphabet signs.

ğŸ‘¤ Target Users

This project is useful for:

ğŸ§ Hearing-impaired people

ğŸ“ Students learning sign language

ğŸ’» Developers exploring computer vision / ML

ğŸ§ª Researchers working on gesture recognition

ğŸ« Educational institutions

ğŸ¤ Anyone wanting to understand basic sign language

â­ High-Level Features

ğŸ“¸ Real-time Hand Tracking
Detects hand movement directly through webcam.

âœ‹ 21 Hand Landmark Extraction
Uses MediaPipe to capture accurate finger & palm positions.

ğŸ§  Machine Learning Classifier
Predicts Aâ€“Z alphabets using a trained Random Forest model.

ğŸ’¾ Custom Dataset Support
You can collect your own gesture images for training.

âš¡ Fast and Efficient
Works smoothly on normal laptops without GPU.

ğŸ” Live Prediction Window
Shows detected hand sign in real-time.
